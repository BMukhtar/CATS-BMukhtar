{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-05-17T06:23:54.819207Z",
     "start_time": "2023-05-17T06:23:52.694155Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/Users/make/.cache/huggingface/datasets/json/default-00ff1c561751f0d8/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b0bbedfad46445ea87e75b894e0bd472"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /Users/make/.cache/huggingface/datasets/json/default-00ff1c561751f0d8/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-47356b4111c2d823.arrow and /Users/make/.cache/huggingface/datasets/json/default-00ff1c561751f0d8/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-bfe10a35ef1eedaa.arrow\n",
      "100%|██████████| 1/1 [00:04<00:00,  4.10s/it]\n",
      "100%|██████████| 1/1 [00:06<00:00,  6.42s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": "Saving the dataset (0/1 shards):   0%|          | 0/100 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bdfe6338df3e48f7a892ada6106c9d03"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Saving the dataset (0/1 shards):   0%|          | 0/100 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ef2388953c3840f0a169f5931ae9c8e3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import datasets\n",
    "from datasets import load_dataset  # huggingface datasets\n",
    "\n",
    "def get_training_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device('mps')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    return device\n",
    "\n",
    "device = get_training_device()\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files='./data/json/dev.json')\n",
    "# take only first 100 examples\n",
    "split_dataset = dataset['train'].train_test_split(test_size=0.1, seed=2357, shuffle=True)\n",
    "split_dataset['val'] = split_dataset.pop('test')  # rename the test split to val\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model_name = 'all-MiniLM-L6-v2'\n",
    "model = SentenceTransformer(model_name, device=device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# we now want to tokenize the dataset. first define the encoding function (gpt2 bpe)\n",
    "def process(example):\n",
    "    embds = model.encode(example['sentences'])\n",
    "    out = {\n",
    "        'id': example['id'],\n",
    "        'labels': example['labels'],\n",
    "        'sentences': example['sentences'],\n",
    "        'embeddings': embds,\n",
    "        'len': len(embds)\n",
    "    }\n",
    "    return out\n",
    "\n",
    "import json\n",
    "jsonl_file = './data/json/dev.jsonl'\n",
    "def save_batch_to_jsonl(data_batch, file_name):\n",
    "    with open(file_name, 'a') as f:  # 'a' for append mode\n",
    "        for data in data_batch:\n",
    "            f.write(json.dumps(data) + '\\n')  # write each json on a new line\n",
    "\n",
    "\n",
    "def preprocess_dataset(dataset: datasets.Dataset):\n",
    "    # for avoiding long time, we process the dataset in batches for parallel processing\n",
    "    batch_size = 512\n",
    "    all_processed = {\n",
    "        'id': [],\n",
    "        'labels': [],\n",
    "        'sentences': [],\n",
    "        'embeddings': [],\n",
    "        'len': []\n",
    "    }\n",
    "    batch_ids = [list(range(i, min(i + batch_size, len(dataset)))) for i in range(0, len(dataset), batch_size)]\n",
    "    for batch_id in tqdm(batch_ids):\n",
    "        batch = dataset.select(batch_id)\n",
    "        all_sentences = []\n",
    "        for example in batch:\n",
    "            all_sentences.extend(example['sentences'])\n",
    "        embs = model.encode(all_sentences)\n",
    "        offset = 0\n",
    "        for example in batch:\n",
    "            next_offset = offset + len(example['sentences'])\n",
    "            embds = embs[offset:offset + next_offset]\n",
    "            offset = next_offset\n",
    "            all_processed['id'].append(example['id'])\n",
    "            all_processed['labels'].append(example['labels'])\n",
    "            all_processed['sentences'].append(example['sentences'])\n",
    "            all_processed['embeddings'].append(embds)\n",
    "            all_processed['len'].append(len(embds))\n",
    "\n",
    "    return datasets.Dataset.from_dict(all_processed)\n",
    "\n",
    "train_dataset = preprocess_dataset(split_dataset['train'].select(range(100)))\n",
    "val_dataset = preprocess_dataset(split_dataset['val'].select(range(100)))\n",
    "\n",
    "# save the tokenized dataset\n",
    "train_dataset.save_to_disk('./data/tokenized_train')\n",
    "val_dataset.save_to_disk('./data/tokenized_val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Convert sentence embeddings and labels for trainable blocks\n",
    "\"\"\"\n",
    "import config\n",
    "import random\n",
    "\n",
    "fake_sent_embedding = model.encode([config.fake_sent])[0]\n",
    "embedding_size = model.get_sentence_embedding_dimension()\n",
    "\n",
    "\n",
    "def create_fake_block(block, lines):\n",
    "    block_fake = block.copy()\n",
    "    random.shuffle(block_fake)\n",
    "    p = random.random()\n",
    "    if p >= 0.5:\n",
    "        for i in range(len(block_fake)):\n",
    "            p = random.random()\n",
    "            if p >= 0.5:\n",
    "                l = lines[random.randint(0, len(lines) - 1)]\n",
    "                block_fake[i] = (l, 0)\n",
    "    return block_fake\n",
    "\n",
    "\n",
    "def create_one_instance(block, lines):\n",
    "    records = []\n",
    "    fake_block = create_fake_block(block, lines)\n",
    "\n",
    "    blocks = list(zip(block, fake_block))\n",
    "    for item in blocks:\n",
    "        real_sentence = item[0][0]\n",
    "        real_label = item[0][1]\n",
    "        fake_sentence = item[1][0]\n",
    "        records.append((real_sentence, fake_sentence, real_label))\n",
    "    return records\n",
    "\n",
    "\n",
    "def get_blocks(dataset, test=False):\n",
    "    all_blocks = []\n",
    "    for example in dataset:\n",
    "        labels = example['labels']\n",
    "        embds = example['embeddings']\n",
    "        raw_blocks = []\n",
    "        stride = 1 if test else config.sent_stride\n",
    "        i = 0\n",
    "        while i < len(labels):\n",
    "            block = [(embd, label) for embd, label in\n",
    "                     zip(embds[i:i + config.sent_window], labels[i:i + config.sent_window])]\n",
    "            if len(block) < config.sent_window:\n",
    "                block.extend([(fake_sent_embedding, 0)] * (config.sent_window - len(block)))\n",
    "            raw_blocks.append(block)\n",
    "            i += stride\n",
    "\n",
    "        if not test:\n",
    "            raw_blocks = raw_blocks[:int(config.perc_blocks_train * len(raw_blocks))]\n",
    "            random.shuffle(raw_blocks)\n",
    "\n",
    "        doc_recs = []\n",
    "        for rb in raw_blocks:\n",
    "            records = create_one_instance(rb, embds)\n",
    "            doc_recs.extend(records)\n",
    "\n",
    "        # save doc_recs to numpy array\n",
    "        all_blocks.extend(doc_recs)\n",
    "    return all_blocks\n",
    "\n",
    "\n",
    "train_blocks = get_blocks(train_dataset)\n",
    "val_blocks = get_blocks(val_dataset, test=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T06:23:56.083357Z",
     "start_time": "2023-05-17T06:23:54.825204Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "writing ./data/processed/train.bin: 3it [00:00, 50.43it/s]\n",
      "writing ./data/processed/val.bin: 103it [00:01, 63.81it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "dtype = np.dtype([('real', np.float32, embedding_size), ('fake', np.float32, embedding_size), ('label', np.int8)])\n",
    "data_dir = \"./data/processed\"\n",
    "\n",
    "\n",
    "def save_to_numpy(blocks, filename):\n",
    "    # create file if it doesn't exist\n",
    "    if not os.path.exists(os.path.dirname(filename)):\n",
    "        os.makedirs(os.path.dirname(filename))\n",
    "    # save all_blocks to numpy array\n",
    "    arr = np.memmap(filename, dtype=dtype, mode='w+', shape=(len(blocks),))\n",
    "\n",
    "    # create batches of blocks for faster write\n",
    "    batch_size = 1024\n",
    "    batches = [blocks[i:i + batch_size] for i in range(0, len(blocks), batch_size)]\n",
    "    for batch_idx, batch in tqdm(enumerate(batches), desc=f'writing {filename}'):\n",
    "        arr_batch = np.array(batch, dtype=dtype)\n",
    "        arr[batch_idx * batch_size: batch_idx * batch_size + len(arr_batch)] = arr_batch\n",
    "    arr.flush()\n",
    "\n",
    "\n",
    "save_to_numpy(train_blocks, os.path.join(data_dir, 'train.bin'))\n",
    "save_to_numpy(val_blocks, os.path.join(data_dir, 'val.bin'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T06:23:57.834736Z",
     "start_time": "2023-05-17T06:23:56.087591Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "# load the numpy array from disk\n",
    "train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=dtype, mode='r')\n",
    "val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=dtype, mode='r')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T06:23:57.847513Z",
     "start_time": "2023-05-17T06:23:57.837736Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "batch_size = 12  # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
    "block_size = config.sent_window\n",
    "\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu'  # for later use in torch.autocast\n",
    "\n",
    "train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=dtype, mode='r')\n",
    "val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=dtype, mode='r')\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in ix:\n",
    "        batch_numpy = data[i:i + block_size]\n",
    "        # given numpy array strides not a multiple of the element byte size. Copy the numpy array to reallocate the memory.\n",
    "        batch_numpy = np.copy(batch_numpy)\n",
    "        x.append(torch.stack(\n",
    "            [torch.from_numpy(np.copy(batch_numpy['real'])), torch.from_numpy(np.copy(batch_numpy['fake']))]))\n",
    "        y.append(torch.from_numpy(batch_numpy['label']))\n",
    "    x = torch.stack(x)\n",
    "    y = torch.stack(y)\n",
    "    if device_type == 'cuda':\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T06:23:57.862846Z",
     "start_time": "2023-05-17T06:23:57.850809Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "sample_batch = get_batch('train')\n",
    "assert sample_batch[0].shape == (batch_size, 2, block_size, embedding_size)\n",
    "assert sample_batch[1].shape == (batch_size, block_size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-17T06:23:57.870382Z",
     "start_time": "2023-05-17T06:23:57.864116Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing a new model from scratch\n",
      "epoch 0 train loss 0.6923 val loss 0.6929\n",
      "saving checkpoint to ./model_checkpoints/ckpt.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/make/mambaforge/envs/cats/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 train loss 0.5964 val loss 0.6112\n",
      "saving checkpoint to ./model_checkpoints/ckpt.pt\n",
      "epoch 20 train loss 0.4508 val loss 0.4728\n",
      "saving checkpoint to ./model_checkpoints/ckpt.pt\n",
      "epoch 30 train loss 0.3379 val loss 0.3755\n",
      "saving checkpoint to ./model_checkpoints/ckpt.pt\n",
      "epoch 40 train loss 0.3404 val loss 0.3117\n",
      "saving checkpoint to ./model_checkpoints/ckpt.pt\n",
      "epoch 50 train loss 0.2992 val loss 0.3039\n",
      "saving checkpoint to ./model_checkpoints/ckpt.pt\n",
      "epoch 60 train loss 0.3060 val loss 0.2987\n",
      "saving checkpoint to ./model_checkpoints/ckpt.pt\n",
      "epoch 70 train loss 0.3059 val loss 0.2984\n",
      "saving checkpoint to ./model_checkpoints/ckpt.pt\n",
      "epoch 80 train loss 0.3016 val loss 0.3031\n",
      "saving checkpoint to ./model_checkpoints/ckpt.pt\n",
      "epoch 90 train loss 0.2847 val loss 0.2963\n",
      "saving checkpoint to ./model_checkpoints/ckpt.pt\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AdamW\n",
    "\n",
    "\n",
    "class TopicSegmentationModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TopicSegmentationModel, self).__init__()\n",
    "\n",
    "        self.linear1 = torch.nn.Linear(block_size * embedding_size, block_size)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(block_size, block_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_real = x[:, 0, :, :].reshape(batch_size, -1)\n",
    "        x = self.linear1(x_real)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "eval_iters = 10\n",
    "eval_interval = 10\n",
    "num_epochs = 100\n",
    "best_val_loss = 1e9\n",
    "ckpt_path = './model_checkpoints/ckpt.pt'\n",
    "num_batches = len(train_data) // batch_size\n",
    "always_save_checkpoint = True\n",
    "init_from = \"scratch\"\n",
    "epoch_offset = 0\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "model = TopicSegmentationModel()\n",
    "model.to(device)\n",
    "\n",
    "if init_from == 'scratch':\n",
    "    # init a new model from scratch\n",
    "    print(\"Initializing a new model from scratch\")\n",
    "    # determine the vocab size we'll use for from-scratch training\n",
    "    optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "else:\n",
    "    print(f\"Resuming training from {ckpt_path}\")\n",
    "    # resume training from a checkpoint.\n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "    # force these config attributes to be equal otherwise we can't even resume training\n",
    "    # the rest of the attributes (e.g. dropout) can stay as desired from command line\n",
    "\n",
    "    # create the model\n",
    "    optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "    epoch = checkpoint['iter_num']\n",
    "\n",
    "    state_dict = checkpoint['model']\n",
    "    epoch_offset = checkpoint['epoch_offset']\n",
    "    # fix the keys of the state dictionary :(\n",
    "    # honestly no idea how checkpoints sometimes get this prefix, have to debug more\n",
    "    unwanted_prefix = '_orig_mod.'\n",
    "    for k, v in list(state_dict.items()):\n",
    "        if k.startswith(unwanted_prefix):\n",
    "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "    model.load_state_dict(state_dict)\n",
    "    iter_num = checkpoint['iter_num']\n",
    "    best_val_loss = checkpoint['best_val_loss']\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            x, y = get_batch(split)\n",
    "            y_hat = model(x)\n",
    "            y = y.float()\n",
    "            loss_value = loss_fn(y_hat, y)\n",
    "            losses[k] = loss_value\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "for epoch in range(epoch_offset, epoch_offset + num_epochs):\n",
    "    # train on training set\n",
    "    # evaluate the loss on train/val sets and write checkpoints\n",
    "    if epoch % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        if losses['val'] < best_val_loss or always_save_checkpoint:\n",
    "            best_val_loss = losses['val']\n",
    "            # print losses\n",
    "            print(f\"epoch {epoch} train loss {losses['train']:.4f} val loss {losses['val']:.4f}\")\n",
    "            if epoch > -1:\n",
    "                checkpoint = {\n",
    "                    'model': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'iter_num': epoch,\n",
    "                    'best_val_loss': best_val_loss,\n",
    "                    'epoch_offset': epoch,\n",
    "                }\n",
    "                print(f\"saving checkpoint to {ckpt_path}\")\n",
    "                torch.save(checkpoint, ckpt_path)\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx in range(num_batches):\n",
    "        x, y = get_batch('train')\n",
    "        optimizer.zero_grad()\n",
    "        y_hat = model(x)\n",
    "        y = y.float()\n",
    "        loss_value = loss_fn(y_hat, y)\n",
    "        train_loss += loss_value.item()\n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "    #\n",
    "    # # evaluate on validation set\n",
    "    # model.eval()\n",
    "    # with torch.no_grad():\n",
    "    #     x, y = get_batch('val')\n",
    "    #     y_hat = model(x)\n",
    "    #     y = y.float()\n",
    "    #     loss_value = loss_fn(y_hat, y)\n",
    "    #     print(f'Epoch {epoch} validation loss: {loss_value.item()}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
