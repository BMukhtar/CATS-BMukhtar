{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-05-19T05:17:56.324888Z",
     "start_time": "2023-05-19T05:17:54.266041Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/Users/make/.cache/huggingface/datasets/json/default-00ff1c561751f0d8/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1d95123fcf9b442185fdf49d8be143f3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /Users/make/.cache/huggingface/datasets/json/default-00ff1c561751f0d8/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-47356b4111c2d823.arrow and /Users/make/.cache/huggingface/datasets/json/default-00ff1c561751f0d8/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-bfe10a35ef1eedaa.arrow\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.27it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.23it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "539bd30765a84932a487ac3eb84cba6b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5c4fd94f829540faaf214a849e20407f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import datasets\n",
    "from datasets import load_dataset  # huggingface datasets\n",
    "\n",
    "import utils\n",
    "import config\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "def sentence_split(text):\n",
    "    return sent_tokenize(text)\n",
    "\n",
    "import os\n",
    "def create_instances(dir_path, title_start = \"========,\", ssplit = True):\n",
    "    files = []\n",
    "    utils.get_files_recursive(dir_path, files)\n",
    "    print(\"Found \" + str(len(files)) + \" text files.\")\n",
    "    res = {\n",
    "        'id': [],\n",
    "        'labels': [],\n",
    "        'sentences': [],\n",
    "    }\n",
    "    for f in tqdm(files):\n",
    "        example = process_document(f, title_start = title_start, ssplit = ssplit)\n",
    "        res['id'].append(example['id'])\n",
    "        res['labels'].append(example['labels'])\n",
    "        res['sentences'].append(example['sentences'])\n",
    "    return res\n",
    "\n",
    "\n",
    "def process_document(path, title_start = \"========,\", forbidden_start = \"***LIST***\", ssplit = True):\n",
    "    lines = ([l for l in utils.load_lines(path) if not l.startswith(forbidden_start)]) if ssplit else (sentence_split(utils.load_txt_file(path)))\n",
    "    labels = []\n",
    "    sentences = []\n",
    "    for i in range(len(lines)):\n",
    "        if lines[i].startswith(title_start):\n",
    "            continue\n",
    "        if (i-1) >= 0 and lines[i-1].startswith(title_start):\n",
    "            sentences.append(lines[i])\n",
    "            labels.append(1)\n",
    "        else:\n",
    "            sentences.append(lines[i])\n",
    "            labels.append(0)\n",
    "\n",
    "    return {\n",
    "        'id': path,\n",
    "        'labels': labels,\n",
    "        'sentences': sentences,\n",
    "    }\n",
    "\n",
    "def get_training_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = 'cuda'\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = 'mps'\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "    return device\n",
    "\n",
    "device = get_training_device()\n",
    "torch.set_default_device(device)\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files='./data/json/dev.json')\n",
    "# take only first 100 examples\n",
    "split_dataset = dataset['train'].train_test_split(test_size=0.1, seed=2357, shuffle=True)\n",
    "split_dataset['val'] = split_dataset.pop('test')  # rename the test split to val\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model_name = 'all-MiniLM-L6-v2'\n",
    "sentence_model = SentenceTransformer(model_name, device=device)\n",
    "\n",
    "# we now want to tokenize the dataset. first define the encoding function (gpt2 bpe)\n",
    "def process(example):\n",
    "    embds = sentence_model.encode(example['sentences'])\n",
    "    out = {\n",
    "        'id': example['id'],\n",
    "        'labels': example['labels'],\n",
    "        'sentences': example['sentences'],\n",
    "        'embeddings': embds,\n",
    "        'len': len(embds)\n",
    "    }\n",
    "    return out\n",
    "\n",
    "def preprocess_list_data(data):\n",
    "    # for avoiding long time, we process the dataset in batches for parallel processing\n",
    "    batch_size = 512\n",
    "    all_processed = {\n",
    "        'id': [],\n",
    "        'labels': [],\n",
    "        'sentences': [],\n",
    "        'embeddings': [],\n",
    "        'len': []\n",
    "    }\n",
    "    batch_ids = [list(range(i, min(i + batch_size, len(data)))) for i in range(0, len(data), batch_size)]\n",
    "    for batch_id in tqdm(batch_ids):\n",
    "        batch = data.select(batch_id)\n",
    "        all_sentences = []\n",
    "        for example in batch:\n",
    "            all_sentences.extend(example['sentences'])\n",
    "        embs = sentence_model.encode(all_sentences)\n",
    "        offset = 0\n",
    "        for example in batch:\n",
    "            next_offset = offset + len(example['sentences'])\n",
    "            embds = embs[offset:offset + next_offset]\n",
    "            offset = next_offset\n",
    "            all_processed['id'].append(example['id'])\n",
    "            all_processed['labels'].append(example['labels'])\n",
    "            all_processed['sentences'].append(example['sentences'])\n",
    "            all_processed['embeddings'].append(embds)\n",
    "            all_processed['len'].append(len(embds))\n",
    "\n",
    "    return all_processed\n",
    "\n",
    "\n",
    "def preprocess_dataset(dataset: datasets.Dataset):\n",
    "    # for avoiding long time, we process the dataset in batches for parallel processing\n",
    "    return datasets.Dataset.from_dict(preprocess_list_data(dataset))\n",
    "\n",
    "train_dataset = preprocess_dataset(split_dataset['train'].select(range(10)))\n",
    "val_dataset = preprocess_dataset(split_dataset['val'].select(range(10)))\n",
    "\n",
    "\n",
    "# save the tokenized dataset\n",
    "train_dataset.save_to_disk('./data/tokenized_train')\n",
    "val_dataset.save_to_disk('./data/tokenized_val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "writing ./data/processed/train.bin: 1it [00:00, 159.95it/s]\n",
      "writing ./data/processed/val.bin: 1it [00:00, 166.45it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Convert sentence embeddings and labels for trainable blocks\n",
    "\"\"\"\n",
    "import config\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "fake_sent_embedding = sentence_model.encode([config.fake_sent])[0]\n",
    "embedding_size = sentence_model.get_sentence_embedding_dimension()\n",
    "\n",
    "\n",
    "def create_fake_block(block, lines):\n",
    "    block_fake = block.copy()\n",
    "    random.shuffle(block_fake)\n",
    "    p = random.random()\n",
    "    if p >= 0.5:\n",
    "        for i in range(len(block_fake)):\n",
    "            p = random.random()\n",
    "            if p >= 0.5:\n",
    "                l = lines[random.randint(0, len(lines) - 1)]\n",
    "                block_fake[i] = (l, 0)\n",
    "    return block_fake\n",
    "\n",
    "\n",
    "def create_one_instance(block, lines):\n",
    "    records = []\n",
    "    fake_block = create_fake_block(block, lines)\n",
    "\n",
    "    blocks = list(zip(block, fake_block))\n",
    "    for item in blocks:\n",
    "        real_sentence = item[0][0]\n",
    "        real_label = item[0][1]\n",
    "        fake_sentence = item[1][0]\n",
    "        records.append((real_sentence, fake_sentence, real_label))\n",
    "    return records\n",
    "\n",
    "\n",
    "def get_blocks(dataset, test=False):\n",
    "    all_blocks = []\n",
    "    for example in dataset:\n",
    "        labels = example['labels']\n",
    "        embds = example['embeddings']\n",
    "        raw_blocks = []\n",
    "        stride = 1 if test else config.sent_stride\n",
    "        i = 0\n",
    "        while i < len(labels):\n",
    "            block = [(embd, label) for embd, label in\n",
    "                     zip(embds[i:i + config.sent_window], labels[i:i + config.sent_window])]\n",
    "            if len(block) < config.sent_window:\n",
    "                block.extend([(fake_sent_embedding, 0)] * (config.sent_window - len(block)))\n",
    "            raw_blocks.append(block)\n",
    "            i += stride\n",
    "\n",
    "        if not test:\n",
    "            raw_blocks = raw_blocks[:int(config.perc_blocks_train * len(raw_blocks))]\n",
    "            random.shuffle(raw_blocks)\n",
    "\n",
    "        doc_recs = []\n",
    "        for rb in raw_blocks:\n",
    "            records = create_one_instance(rb, embds)\n",
    "            doc_recs.extend(records)\n",
    "\n",
    "        # save doc_recs to numpy array\n",
    "        all_blocks.extend(doc_recs)\n",
    "    return all_blocks\n",
    "\n",
    "dtype = np.dtype([('real', np.float32, embedding_size), ('fake', np.float32, embedding_size), ('label', np.int8)])\n",
    "data_dir = \"./data/processed\"\n",
    "\n",
    "\n",
    "def save_to_numpy(blocks, filename):\n",
    "    # create file if it doesn't exist\n",
    "    if not os.path.exists(os.path.dirname(filename)):\n",
    "        os.makedirs(os.path.dirname(filename))\n",
    "    # save all_blocks to numpy array\n",
    "    arr = np.memmap(filename, dtype=dtype, mode='w+', shape=(len(blocks),))\n",
    "\n",
    "    # create batches of blocks for faster write\n",
    "    batch_size = 1024\n",
    "    batches = [blocks[i:i + batch_size] for i in range(0, len(blocks), batch_size)]\n",
    "    for batch_idx, batch in tqdm(enumerate(batches), desc=f'writing {filename}'):\n",
    "        arr_batch = np.array(batch, dtype=dtype)\n",
    "        arr[batch_idx * batch_size: batch_idx * batch_size + len(arr_batch)] = arr_batch\n",
    "    arr.flush()\n",
    "\n",
    "\n",
    "def dataset_to_numpy(dataset, name):\n",
    "    blocks = get_blocks(dataset)\n",
    "    save_to_numpy(blocks, os.path.join(data_dir, f'{name}.bin'))\n",
    "    return np.memmap(os.path.join(data_dir, f'{name}.bin'), dtype=dtype, mode='r')\n",
    "\n",
    "\n",
    "train_data = dataset_to_numpy(train_dataset, 'train')\n",
    "val_data = dataset_to_numpy(val_dataset, 'val')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-19T05:17:56.568790Z",
     "start_time": "2023-05-19T05:17:56.325166Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "batch_size = 12  # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
    "block_size = config.sent_window\n",
    "\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu'  # for later use in torch.autocast\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in ix:\n",
    "        batch_numpy = data[i:i + block_size]\n",
    "        # given numpy array strides not a multiple of the element byte size. Copy the numpy array to reallocate the memory.\n",
    "        batch_numpy = np.copy(batch_numpy)\n",
    "        x.append(torch.stack(\n",
    "            [torch.from_numpy(np.copy(batch_numpy['real'])), torch.from_numpy(np.copy(batch_numpy['fake']))]))\n",
    "        y.append(torch.from_numpy(batch_numpy['label']))\n",
    "    x = torch.stack(x)\n",
    "    y = torch.stack(y)\n",
    "    if device_type == 'cuda':\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-19T05:17:56.587208Z",
     "start_time": "2023-05-19T05:17:56.570284Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "sample_batch = get_batch('train')\n",
    "assert sample_batch[0].shape == (batch_size, 2, block_size, embedding_size)\n",
    "assert sample_batch[1].shape == (batch_size, block_size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-19T05:17:56.587554Z",
     "start_time": "2023-05-19T05:17:56.572704Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing a new model from scratch\n",
      "epoch 0 train loss 0.6873 val loss 0.6884\n",
      "saving checkpoint to ./model_checkpoints/ckpt.pt\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AdamW\n",
    "\n",
    "\n",
    "class TopicSegmentationModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TopicSegmentationModel, self).__init__()\n",
    "\n",
    "        self.linear1 = torch.nn.Linear(block_size * embedding_size, block_size)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(block_size, block_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_real = x[:, 0, :, :].reshape(batch_size, -1)\n",
    "        x = self.linear1(x_real)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "eval_iters = 10\n",
    "eval_interval = 10\n",
    "num_epochs = 1\n",
    "best_val_loss = 1e9\n",
    "ckpt_path = './model_checkpoints/ckpt.pt'\n",
    "num_batches = len(train_data) // batch_size\n",
    "always_save_checkpoint = True\n",
    "init_from = \"scratch\"\n",
    "epoch_offset = 0\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "model = TopicSegmentationModel()\n",
    "sentence_model.to(device)\n",
    "\n",
    "if init_from == 'scratch':\n",
    "    # init a new model from scratch\n",
    "    print(\"Initializing a new model from scratch\")\n",
    "    # determine the vocab size we'll use for from-scratch training\n",
    "    optimizer = AdamW(sentence_model.parameters(), lr=1e-5)\n",
    "else:\n",
    "    print(f\"Resuming training from {ckpt_path}\")\n",
    "    # resume training from a checkpoint.\n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "    # force these config attributes to be equal otherwise we can't even resume training\n",
    "    # the rest of the attributes (e.g. dropout) can stay as desired from command line\n",
    "\n",
    "    # create the model\n",
    "    optimizer = AdamW(sentence_model.parameters(), lr=1e-5)\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "    epoch = checkpoint['iter_num']\n",
    "\n",
    "    state_dict = checkpoint['model']\n",
    "    epoch_offset = checkpoint['epoch_offset']\n",
    "    # fix the keys of the state dictionary :(\n",
    "    # honestly no idea how checkpoints sometimes get this prefix, have to debug more\n",
    "    unwanted_prefix = '_orig_mod.'\n",
    "    for k, v in list(state_dict.items()):\n",
    "        if k.startswith(unwanted_prefix):\n",
    "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "    model.load_state_dict(state_dict)\n",
    "    iter_num = checkpoint['iter_num']\n",
    "    best_val_loss = checkpoint['best_val_loss']\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    sentence_model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            x, y = get_batch(split)\n",
    "            y_hat = model(x)\n",
    "            y = y.float()\n",
    "            loss_value = loss_fn(y_hat, y)\n",
    "            losses[k] = loss_value\n",
    "        out[split] = losses.mean()\n",
    "    sentence_model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "for epoch in range(epoch_offset, epoch_offset + num_epochs):\n",
    "    # train on training set\n",
    "    # evaluate the loss on train/val sets and write checkpoints\n",
    "    if epoch % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        if losses['val'] < best_val_loss or always_save_checkpoint:\n",
    "            best_val_loss = losses['val']\n",
    "            # print losses\n",
    "            print(f\"epoch {epoch} train loss {losses['train']:.4f} val loss {losses['val']:.4f}\")\n",
    "            if epoch > -1:\n",
    "                checkpoint = {\n",
    "                    'model': sentence_model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'iter_num': epoch,\n",
    "                    'best_val_loss': best_val_loss,\n",
    "                    'epoch_offset': epoch,\n",
    "                }\n",
    "                print(f\"saving checkpoint to {ckpt_path}\")\n",
    "                torch.save(checkpoint, ckpt_path)\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx in range(num_batches):\n",
    "        x, y = get_batch('train')\n",
    "        optimizer.zero_grad()\n",
    "        y_hat = model(x)\n",
    "        y = y.float()\n",
    "        loss_value = loss_fn(y_hat, y)\n",
    "        train_loss += loss_value.item()\n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "    #\n",
    "    # # evaluate on validation set\n",
    "    # model.eval()\n",
    "    # with torch.no_grad():\n",
    "    #     x, y = get_batch('val')\n",
    "    #     y_hat = model(x)\n",
    "    #     y = y.float()\n",
    "    #     loss_value = loss_fn(y_hat, y)\n",
    "    #     print(f'Epoch {epoch} validation loss: {loss_value.item()}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-19T05:17:57.162324Z",
     "start_time": "2023-05-19T05:17:56.586113Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/input\t0\n",
      "Found 50 text files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 7466.10it/s]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.60s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": "Saving the dataset (0/1 shards):   0%|          | 0/50 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "49ada10acaf045d79e159e24f5b97268"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "writing ./data/processed/test.bin: 2it [00:00, 51.56it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot compare structured or void to non-void arrays.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[32], line 87\u001B[0m\n\u001B[1;32m     85\u001B[0m test_dataset \u001B[38;5;241m=\u001B[39m preprocess_dataset(datasets\u001B[38;5;241m.\u001B[39mDataset\u001B[38;5;241m.\u001B[39mfrom_dict(create_instances(dir_path\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./data/input\u001B[39m\u001B[38;5;124m'\u001B[39m)))\n\u001B[1;32m     86\u001B[0m test_dataset\u001B[38;5;241m.\u001B[39msave_to_disk(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./data/tokenized_test\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m---> 87\u001B[0m \u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtest_dataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m./data/output\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[32], line 28\u001B[0m, in \u001B[0;36mpredict\u001B[0;34m(dataset, output_dir)\u001B[0m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpredict\u001B[39m(dataset, output_dir):\n\u001B[1;32m     27\u001B[0m     test_data \u001B[38;5;241m=\u001B[39m dataset_to_numpy(dataset, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtest\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m---> 28\u001B[0m     \u001B[43mget_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtest_data\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     29\u001B[0m     res \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mforward(blocks)\n\u001B[1;32m     31\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDocuments to segment: \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mlen\u001B[39m(test_texts[\u001B[38;5;241m0\u001B[39m])))\n",
      "Cell \u001B[0;32mIn[26], line 7\u001B[0m, in \u001B[0;36mget_batch\u001B[0;34m(split)\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_batch\u001B[39m(split):\n\u001B[0;32m----> 7\u001B[0m     data \u001B[38;5;241m=\u001B[39m train_data \u001B[38;5;28;01mif\u001B[39;00m \u001B[43msplit\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtrain\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m \u001B[38;5;28;01melse\u001B[39;00m val_data\n\u001B[1;32m      8\u001B[0m     ix \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrandint(\u001B[38;5;28mlen\u001B[39m(data) \u001B[38;5;241m-\u001B[39m block_size, (batch_size,))\n\u001B[1;32m      9\u001B[0m     x \u001B[38;5;241m=\u001B[39m []\n",
      "\u001B[0;31mTypeError\u001B[0m: Cannot compare structured or void to non-void arrays."
     ]
    }
   ],
   "source": [
    "batch_size = 12  # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
    "block_size = config.sent_window\n",
    "\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu'  # for later use in torch.autocast\n",
    "\n",
    "def get_test_batch(data):\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in ix:\n",
    "        batch_numpy = data[i:i + block_size]\n",
    "        # given numpy array strides not a multiple of the element byte size. Copy the numpy array to reallocate the memory.\n",
    "        batch_numpy = np.copy(batch_numpy)\n",
    "        x.append(torch.stack(\n",
    "            [torch.from_numpy(np.copy(batch_numpy['real'])), torch.from_numpy(np.copy(batch_numpy['fake']))]))\n",
    "        y.append(torch.from_numpy(batch_numpy['label']))\n",
    "    x = torch.stack(x)\n",
    "    y = torch.stack(y)\n",
    "    if device_type == 'cuda':\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "def predict(dataset, output_dir):\n",
    "    test_data = dataset_to_numpy(dataset, 'test')\n",
    "    get_batch(test_data)\n",
    "    res = model.forward(blocks)\n",
    "\n",
    "    print(\"Documents to segment: \" + str(len(test_texts[0])))\n",
    "    flat_blocks = []\n",
    "    for x in test_texts[0]:\n",
    "        print(len(x[1]))\n",
    "        flat_blocks.extend(x[1])\n",
    "\n",
    "    print(\"Number of prediction blocks: \" + str(len(flat_blocks)))\n",
    "\n",
    "    print(\"Predicting with the model (this may take a while, depending on the number of documents)...\")\n",
    "    res_list = list(res)\n",
    "    print(\"Predictions completed.\")\n",
    "\n",
    "    thold = 0.3 if config.MODEL_TYPE == \"cats\" else 0.5\n",
    "\n",
    "    glob_cntr = 0\n",
    "    docs = test_texts[0]\n",
    "\n",
    "    agg_docs = []\n",
    "\n",
    "    for i in range(len(docs)):\n",
    "        fname = docs[i][0]\n",
    "        if i % 1000 == 1:\n",
    "            print(fname)\n",
    "            print(str(i) + \" of \" + str(len(docs)) + \" documents...\")\n",
    "        blocks = docs[i][1]\n",
    "        preds_blocks = res_list[glob_cntr: glob_cntr + len(blocks)]\n",
    "        glob_cntr += len(blocks)\n",
    "\n",
    "        sent_scores = [(b[0][0], b[0][1], []) for b in blocks]\n",
    "        for b_ind in range(len(blocks)):\n",
    "            for relb_ind in range(len(blocks[b_ind])):\n",
    "                if blocks[b_ind][relb_ind][0] == config.fake_sent:\n",
    "                    break\n",
    "                else:\n",
    "                    sent_ind = b_ind + relb_ind\n",
    "                    score = preds_blocks[b_ind][relb_ind][1]\n",
    "                    sent_scores[sent_ind][2].append(score)\n",
    "        agg_sent_scores = [(x[0], x[1], np.mean(x[2]), (1 if np.mean(x[2]) >= thold else 0)) for x in sent_scores]\n",
    "        agg_docs.append(agg_sent_scores)\n",
    "\n",
    "    # printing out predictions\n",
    "    docnames = [x[0] for x in docs]\n",
    "    print(\"Storing segmented texts...\")\n",
    "    docscores = zip(docnames, agg_docs)\n",
    "    for name, sentscores in docscores:\n",
    "        print(\"Document: \" + name)\n",
    "        lines = []\n",
    "        for s in sentscores:\n",
    "            if s[2] >= thold:\n",
    "                lines.append(config.seg_start)\n",
    "            lines.append(s[0] + \"\\t\" + str(s[2]) if write_pred_score else s[0])\n",
    "        utils.write_list(os.path.join(output_dir, name.split(\"/\")[-1] + \".seg\"), lines)\n",
    "    print(\"Stored.\")\n",
    "\n",
    "test_dataset = preprocess_dataset(datasets.Dataset.from_dict(create_instances(dir_path='./data/input')))\n",
    "test_dataset.save_to_disk('./data/tokenized_test')\n",
    "predict(test_dataset, \"./data/output\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
