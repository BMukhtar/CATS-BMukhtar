{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-05-15T04:19:33.103018Z",
     "start_time": "2023-05-15T04:19:29.532029Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/Users/make/.cache/huggingface/datasets/json/default-00ff1c561751f0d8/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c613641ae7354d58a6c21e65694d8f17"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /Users/make/.cache/huggingface/datasets/json/default-00ff1c561751f0d8/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-47356b4111c2d823.arrow and /Users/make/.cache/huggingface/datasets/json/default-00ff1c561751f0d8/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-bfe10a35ef1eedaa.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": "tokenizing the splits:   0%|          | 0/100 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8002359bfb2040bb863b9914c1af7109"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "tokenizing the splits:   0%|          | 0/100 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "492f6764c5624901b5b8ca4205f86e9e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Saving the dataset (0/1 shards):   0%|          | 0/100 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "50fc591effcf4de99f97a358fb5cf3be"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Saving the dataset (0/1 shards):   0%|          | 0/100 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e6f7cd1c17f742c28ab4d39839df1d55"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset  # huggingface datasets\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files='./data/json/dev.json')\n",
    "# take only first 100 examples\n",
    "split_dataset = dataset['train'].train_test_split(test_size=0.1, seed=2357, shuffle=True)\n",
    "split_dataset['val'] = split_dataset.pop('test')  # rename the test split to val\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model_name = 'all-MiniLM-L6-v2'\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# we now want to tokenize the dataset. first define the encoding function (gpt2 bpe)\n",
    "def process(example):\n",
    "    embds = model.encode(example['sentences'])\n",
    "    out = {'id': example['id'], 'labels': example['labels'], 'embeddings': embds, 'len': len(embds)}\n",
    "    return out\n",
    "\n",
    "\n",
    "def preprocess_dataset(dataset: datasets.Dataset):\n",
    "    # for avoiding memory errors, we process the dataset in batches\n",
    "    batch_size = 100\n",
    "    batch_ids = [list(range(i, min(i + batch_size, len(dataset)))) for i in range(0, len(dataset), batch_size)]\n",
    "    for batch_id in batch_ids:\n",
    "        batch = dataset.select(batch_id)\n",
    "        # get sentences from batch for transformer\n",
    "        batch = batch.map(\n",
    "            process,\n",
    "            # remove_columns=['sentences'],\n",
    "            desc=\"tokenizing the splits\",\n",
    "            num_proc=1,\n",
    "        )\n",
    "        dataset.remove_columns(['sentences'])\n",
    "        dataset = dataset.concat(batch)\n",
    "\n",
    "# tokenize the dataset\n",
    "train_dataset = split_dataset['train'].select(range(100)).map(\n",
    "    process,\n",
    "    # remove_columns=['sentences'],\n",
    "    desc=\"tokenizing the splits\",\n",
    "    num_proc=1,\n",
    ")\n",
    "val_dataset = split_dataset['val'].select(range(100)).map(\n",
    "    process,\n",
    "    # remove_columns=['sentences'],\n",
    "    desc=\"tokenizing the splits\",\n",
    "    num_proc=1,\n",
    ")\n",
    "# save the tokenized dataset\n",
    "train_dataset.save_to_disk('./data/tokenized_train')\n",
    "val_dataset.save_to_disk('./data/tokenized_val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Convert sentence embeddings and labels for trainable blocks\n",
    "\"\"\"\n",
    "import config\n",
    "import random\n",
    "fake_sent_embedding = model.encode([config.fake_sent])[0]\n",
    "embedding_size = model.get_sentence_embedding_dimension()\n",
    "\n",
    "\n",
    "def create_fake_block(block, lines):\n",
    "    block_fake = block.copy()\n",
    "    random.shuffle(block_fake)\n",
    "    p = random.random()\n",
    "    if p >= 0.5:\n",
    "        for i in range(len(block_fake)):\n",
    "            p = random.random()\n",
    "            if p >= 0.5:\n",
    "                l = lines[random.randint(0, len(lines) - 1)]\n",
    "                block_fake[i] = (l, 0)\n",
    "    return block_fake\n",
    "\n",
    "def create_one_instance(block, lines):\n",
    "    records = []\n",
    "    fake_block = create_fake_block(block, lines)\n",
    "\n",
    "    blocks = list(zip(block, fake_block))\n",
    "    for item in blocks:\n",
    "        real_sentence = item[0][0]\n",
    "        real_label = item[0][1]\n",
    "        fake_sentence = item[1][0]\n",
    "        records.append((real_sentence,fake_sentence, real_label))\n",
    "    return records\n",
    "\n",
    "def get_blocks(dataset, test = False):\n",
    "    all_blocks = []\n",
    "    for example in dataset:\n",
    "        labels = example['labels']\n",
    "        embds = example['embeddings']\n",
    "        raw_blocks = []\n",
    "        stride = 1 if test else config.sent_stride\n",
    "        i = 0\n",
    "        while i < len(labels):\n",
    "            block = [(embd, label) for embd, label in zip(embds[i:i + config.sent_window], labels[i:i + config.sent_window])]\n",
    "            if len(block) < config.sent_window:\n",
    "                block.extend([(fake_sent_embedding, 0)] * (config.sent_window - len(block)))\n",
    "            raw_blocks.append(block)\n",
    "            i += stride\n",
    "\n",
    "        if not test:\n",
    "            raw_blocks = raw_blocks[:int(config.perc_blocks_train * len(raw_blocks))]\n",
    "            random.shuffle(raw_blocks)\n",
    "\n",
    "        doc_recs = []\n",
    "        for rb in raw_blocks:\n",
    "            records = create_one_instance(rb, embds)\n",
    "            doc_recs.extend(records)\n",
    "\n",
    "        # save doc_recs to numpy array\n",
    "        all_blocks.extend(doc_recs)\n",
    "    return all_blocks\n",
    "\n",
    "train_blocks = get_blocks(train_dataset)\n",
    "val_blocks = get_blocks(val_dataset, test=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-15T04:19:33.434435Z",
     "start_time": "2023-05-15T04:19:33.104113Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "writing ./data/processed/train.bin: 3it [00:00, 47.44it/s]\n",
      "writing ./data/processed/val.bin: 103it [00:01, 64.24it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "dtype = np.dtype([('real', np.float32, embedding_size), ('fake', np.float32, embedding_size), ('label', np.int8)])\n",
    "data_dir = \"./data/processed\"\n",
    "def save_to_numpy(blocks, filename):\n",
    "    # create file if it doesn't exist\n",
    "    if not os.path.exists(os.path.dirname(filename)):\n",
    "        os.makedirs(os.path.dirname(filename))\n",
    "    # save all_blocks to numpy array\n",
    "    arr = np.memmap(filename, dtype=dtype, mode='w+', shape=(len(blocks),))\n",
    "\n",
    "    # create batches of blocks for faster write\n",
    "    batch_size = 1024\n",
    "    batches = [blocks[i:i + batch_size] for i in range(0, len(blocks), batch_size)]\n",
    "    for batch_idx, batch in tqdm(enumerate(batches), desc=f'writing {filename}'):\n",
    "        arr_batch = np.array(batch, dtype=dtype)\n",
    "        arr[batch_idx * batch_size : batch_idx * batch_size + len(arr_batch)] = arr_batch\n",
    "    arr.flush()\n",
    "\n",
    "save_to_numpy(train_blocks, os.path.join(data_dir, 'train.bin'))\n",
    "save_to_numpy(val_blocks, os.path.join(data_dir, 'val.bin'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-15T04:19:33.603243Z",
     "start_time": "2023-05-15T04:19:33.439526Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# load the numpy array from disk\n",
    "train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=dtype, mode='r')\n",
    "val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=dtype, mode='r')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-15T04:19:33.613513Z",
     "start_time": "2023-05-15T04:19:33.604285Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "batch_size = 12 # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
    "block_size = config.sent_window\n",
    "device = 'cpu' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "\n",
    "\n",
    "train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=dtype, mode='r')\n",
    "val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=dtype, mode='r')\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in ix:\n",
    "        batch_numpy = data[i:i+block_size]\n",
    "        # given numpy array strides not a multiple of the element byte size. Copy the numpy array to reallocate the memory.\n",
    "        batch_numpy = np.copy(batch_numpy)\n",
    "        x.append(torch.stack([torch.from_numpy( np.copy(batch_numpy['real'])), torch.from_numpy( np.copy(batch_numpy['fake']))]))\n",
    "        y.append(torch.from_numpy(batch_numpy['label']))\n",
    "    x = torch.stack(x)\n",
    "    y = torch.stack(y)\n",
    "    if device_type == 'cuda':\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-15T04:30:20.829490Z",
     "start_time": "2023-05-15T04:30:20.807812Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "sample_batch = get_batch('train')\n",
    "assert sample_batch[0].shape == (batch_size, 2, block_size, embedding_size)\n",
    "assert sample_batch[1].shape == (batch_size, block_size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train loss: 0.6882 validation loss: 0.6859\n",
      "Epoch 1, train loss: 0.6814 validation loss: 0.6796\n",
      "Epoch 2, train loss: 0.6731 validation loss: 0.6725\n",
      "Epoch 3, train loss: 0.6639 validation loss: 0.6636\n",
      "Epoch 4, train loss: 0.6530 validation loss: 0.6533\n",
      "Epoch 5, train loss: 0.6419 validation loss: 0.6428\n",
      "Epoch 6, train loss: 0.6284 validation loss: 0.6325\n",
      "Epoch 7, train loss: 0.6131 validation loss: 0.6196\n",
      "Epoch 8, train loss: 0.6002 validation loss: 0.6066\n",
      "Epoch 9, train loss: 0.5857 validation loss: 0.5938\n",
      "Epoch 10, train loss: 0.5688 validation loss: 0.5805\n",
      "Epoch 11, train loss: 0.5555 validation loss: 0.5672\n",
      "Epoch 12, train loss: 0.5408 validation loss: 0.5528\n",
      "Epoch 13, train loss: 0.5262 validation loss: 0.5408\n",
      "Epoch 14, train loss: 0.5132 validation loss: 0.5268\n",
      "Epoch 15, train loss: 0.5011 validation loss: 0.5148\n",
      "Epoch 16, train loss: 0.4849 validation loss: 0.5031\n",
      "Epoch 17, train loss: 0.4718 validation loss: 0.4901\n",
      "Epoch 18, train loss: 0.4628 validation loss: 0.4808\n",
      "Epoch 19, train loss: 0.4514 validation loss: 0.4684\n",
      "Epoch 20, train loss: 0.4381 validation loss: 0.4576\n",
      "Epoch 21, train loss: 0.4262 validation loss: 0.4456\n",
      "Epoch 22, train loss: 0.4173 validation loss: 0.4381\n",
      "Epoch 23, train loss: 0.4098 validation loss: 0.4274\n",
      "Epoch 24, train loss: 0.4000 validation loss: 0.4197\n",
      "Epoch 25, train loss: 0.3958 validation loss: 0.4121\n",
      "Epoch 26, train loss: 0.3853 validation loss: 0.4043\n",
      "Epoch 27, train loss: 0.3771 validation loss: 0.3922\n",
      "Epoch 28, train loss: 0.3724 validation loss: 0.3873\n",
      "Epoch 29, train loss: 0.3638 validation loss: 0.3813\n",
      "Epoch 30, train loss: 0.3602 validation loss: 0.3760\n",
      "Epoch 31, train loss: 0.3560 validation loss: 0.3702\n",
      "Epoch 32, train loss: 0.3504 validation loss: 0.3675\n",
      "Epoch 33, train loss: 0.3459 validation loss: 0.3615\n",
      "Epoch 34, train loss: 0.3415 validation loss: 0.3546\n",
      "Epoch 35, train loss: 0.3411 validation loss: 0.3568\n",
      "Epoch 36, train loss: 0.3363 validation loss: 0.3468\n",
      "Epoch 37, train loss: 0.3370 validation loss: 0.3504\n",
      "Epoch 38, train loss: 0.3356 validation loss: 0.3421\n",
      "Epoch 39, train loss: 0.3325 validation loss: 0.3380\n",
      "Epoch 40, train loss: 0.3259 validation loss: 0.3349\n",
      "Epoch 41, train loss: 0.3256 validation loss: 0.3313\n",
      "Epoch 42, train loss: 0.3266 validation loss: 0.3318\n",
      "Epoch 43, train loss: 0.3270 validation loss: 0.3309\n",
      "Epoch 44, train loss: 0.3238 validation loss: 0.3255\n",
      "Epoch 45, train loss: 0.3194 validation loss: 0.3275\n",
      "Epoch 46, train loss: 0.3147 validation loss: 0.3264\n",
      "Epoch 47, train loss: 0.3190 validation loss: 0.3236\n",
      "Epoch 48, train loss: 0.3167 validation loss: 0.3257\n",
      "Epoch 49, train loss: 0.3146 validation loss: 0.3181\n",
      "Epoch 50, train loss: 0.3176 validation loss: 0.3193\n",
      "Epoch 51, train loss: 0.3158 validation loss: 0.3205\n",
      "Epoch 52, train loss: 0.3158 validation loss: 0.3244\n",
      "Epoch 53, train loss: 0.3102 validation loss: 0.3157\n",
      "Epoch 54, train loss: 0.3130 validation loss: 0.3147\n",
      "Epoch 55, train loss: 0.3163 validation loss: 0.3171\n",
      "Epoch 56, train loss: 0.3133 validation loss: 0.3157\n",
      "Epoch 57, train loss: 0.3089 validation loss: 0.3095\n",
      "Epoch 58, train loss: 0.3093 validation loss: 0.3128\n",
      "Epoch 59, train loss: 0.3059 validation loss: 0.3187\n",
      "Epoch 60, train loss: 0.3093 validation loss: 0.3098\n",
      "Epoch 61, train loss: 0.3064 validation loss: 0.3124\n",
      "Epoch 62, train loss: 0.3086 validation loss: 0.3153\n",
      "Epoch 63, train loss: 0.3023 validation loss: 0.3134\n",
      "Epoch 64, train loss: 0.3113 validation loss: 0.3100\n",
      "Epoch 65, train loss: 0.3051 validation loss: 0.3136\n",
      "Epoch 66, train loss: 0.3070 validation loss: 0.3139\n",
      "Epoch 67, train loss: 0.3093 validation loss: 0.3098\n",
      "Epoch 68, train loss: 0.3055 validation loss: 0.3101\n",
      "Epoch 69, train loss: 0.3043 validation loss: 0.3104\n",
      "Epoch 70, train loss: 0.3013 validation loss: 0.3070\n",
      "Epoch 71, train loss: 0.3012 validation loss: 0.3076\n",
      "Epoch 72, train loss: 0.3017 validation loss: 0.3116\n",
      "Epoch 73, train loss: 0.3034 validation loss: 0.3087\n",
      "Epoch 74, train loss: 0.3017 validation loss: 0.3049\n",
      "Epoch 75, train loss: 0.3057 validation loss: 0.3114\n",
      "Epoch 76, train loss: 0.3033 validation loss: 0.3105\n",
      "Epoch 77, train loss: 0.3046 validation loss: 0.3105\n",
      "Epoch 78, train loss: 0.2976 validation loss: 0.3094\n",
      "Epoch 79, train loss: 0.3024 validation loss: 0.3092\n",
      "Epoch 80, train loss: 0.3000 validation loss: 0.3074\n",
      "Epoch 81, train loss: 0.2999 validation loss: 0.3095\n",
      "Epoch 82, train loss: 0.3049 validation loss: 0.3042\n",
      "Epoch 83, train loss: 0.3026 validation loss: 0.3097\n",
      "Epoch 84, train loss: 0.2931 validation loss: 0.3105\n",
      "Epoch 85, train loss: 0.2980 validation loss: 0.3072\n",
      "Epoch 86, train loss: 0.2963 validation loss: 0.3057\n",
      "Epoch 87, train loss: 0.3073 validation loss: 0.3084\n",
      "Epoch 88, train loss: 0.3018 validation loss: 0.3080\n",
      "Epoch 89, train loss: 0.2955 validation loss: 0.3039\n",
      "Epoch 90, train loss: 0.2979 validation loss: 0.3064\n",
      "Epoch 91, train loss: 0.3011 validation loss: 0.3106\n",
      "Epoch 92, train loss: 0.2988 validation loss: 0.3075\n",
      "Epoch 93, train loss: 0.3004 validation loss: 0.3082\n",
      "Epoch 94, train loss: 0.2929 validation loss: 0.3037\n",
      "Epoch 95, train loss: 0.2999 validation loss: 0.3077\n",
      "Epoch 96, train loss: 0.2939 validation loss: 0.3095\n",
      "Epoch 97, train loss: 0.2946 validation loss: 0.3078\n",
      "Epoch 98, train loss: 0.2966 validation loss: 0.3042\n",
      "Epoch 99, train loss: 0.2945 validation loss: 0.3023\n",
      "Epoch 100, train loss: 0.2951 validation loss: 0.3015\n",
      "Epoch 101, train loss: 0.2918 validation loss: 0.3058\n",
      "Epoch 102, train loss: 0.2973 validation loss: 0.3045\n",
      "Epoch 103, train loss: 0.3006 validation loss: 0.3046\n",
      "Epoch 104, train loss: 0.2937 validation loss: 0.3050\n",
      "Epoch 105, train loss: 0.2962 validation loss: 0.3021\n",
      "Epoch 106, train loss: 0.2956 validation loss: 0.3066\n",
      "Epoch 107, train loss: 0.2889 validation loss: 0.3040\n",
      "Epoch 108, train loss: 0.2984 validation loss: 0.3072\n",
      "Epoch 109, train loss: 0.2938 validation loss: 0.3082\n",
      "Epoch 110, train loss: 0.2954 validation loss: 0.3042\n",
      "Epoch 111, train loss: 0.2970 validation loss: 0.3085\n",
      "Epoch 112, train loss: 0.2996 validation loss: 0.2995\n",
      "Epoch 113, train loss: 0.2950 validation loss: 0.3045\n",
      "Epoch 114, train loss: 0.2942 validation loss: 0.3016\n",
      "Epoch 115, train loss: 0.2942 validation loss: 0.3061\n",
      "Epoch 116, train loss: 0.2973 validation loss: 0.3049\n",
      "Epoch 117, train loss: 0.2956 validation loss: 0.3053\n",
      "Epoch 118, train loss: 0.2907 validation loss: 0.3044\n",
      "Epoch 119, train loss: 0.2939 validation loss: 0.3040\n",
      "Epoch 120, train loss: 0.2911 validation loss: 0.3032\n",
      "Epoch 121, train loss: 0.2895 validation loss: 0.2993\n",
      "Epoch 122, train loss: 0.2923 validation loss: 0.3051\n",
      "Epoch 123, train loss: 0.2940 validation loss: 0.3070\n",
      "Epoch 124, train loss: 0.2913 validation loss: 0.3059\n",
      "Epoch 125, train loss: 0.2906 validation loss: 0.3038\n",
      "Epoch 126, train loss: 0.2896 validation loss: 0.3048\n",
      "Epoch 127, train loss: 0.2932 validation loss: 0.3014\n",
      "Epoch 128, train loss: 0.2951 validation loss: 0.3021\n",
      "Epoch 129, train loss: 0.2881 validation loss: 0.3056\n",
      "Epoch 130, train loss: 0.2945 validation loss: 0.3044\n",
      "Epoch 131, train loss: 0.2901 validation loss: 0.3053\n",
      "Epoch 132, train loss: 0.2893 validation loss: 0.3011\n",
      "Epoch 133, train loss: 0.2933 validation loss: 0.3044\n",
      "Epoch 134, train loss: 0.2903 validation loss: 0.3075\n",
      "Epoch 135, train loss: 0.2856 validation loss: 0.3035\n",
      "Epoch 136, train loss: 0.2895 validation loss: 0.3044\n",
      "Epoch 137, train loss: 0.2890 validation loss: 0.3046\n",
      "Epoch 138, train loss: 0.2889 validation loss: 0.3013\n",
      "Epoch 139, train loss: 0.2880 validation loss: 0.3108\n",
      "Epoch 140, train loss: 0.2894 validation loss: 0.3050\n",
      "Epoch 141, train loss: 0.2923 validation loss: 0.3058\n",
      "Epoch 142, train loss: 0.2862 validation loss: 0.3010\n",
      "Epoch 143, train loss: 0.2904 validation loss: 0.3017\n",
      "Epoch 144, train loss: 0.2901 validation loss: 0.3059\n",
      "Epoch 145, train loss: 0.2848 validation loss: 0.3011\n",
      "Epoch 146, train loss: 0.2860 validation loss: 0.3029\n",
      "Epoch 147, train loss: 0.2916 validation loss: 0.3083\n",
      "Epoch 148, train loss: 0.2876 validation loss: 0.3039\n",
      "Epoch 149, train loss: 0.2875 validation loss: 0.3049\n",
      "Epoch 150, train loss: 0.2808 validation loss: 0.2999\n",
      "Epoch 151, train loss: 0.2836 validation loss: 0.3015\n",
      "Epoch 152, train loss: 0.2870 validation loss: 0.3013\n",
      "Epoch 153, train loss: 0.2813 validation loss: 0.3034\n",
      "Epoch 154, train loss: 0.2853 validation loss: 0.3062\n",
      "Epoch 155, train loss: 0.2852 validation loss: 0.3017\n",
      "Epoch 156, train loss: 0.2833 validation loss: 0.2988\n",
      "Epoch 157, train loss: 0.2850 validation loss: 0.3043\n",
      "Epoch 158, train loss: 0.2872 validation loss: 0.3005\n",
      "Epoch 159, train loss: 0.2906 validation loss: 0.3059\n",
      "Epoch 160, train loss: 0.2882 validation loss: 0.3032\n",
      "Epoch 161, train loss: 0.2858 validation loss: 0.3042\n",
      "Epoch 162, train loss: 0.2871 validation loss: 0.3034\n",
      "Epoch 163, train loss: 0.2841 validation loss: 0.3006\n",
      "Epoch 164, train loss: 0.2807 validation loss: 0.3030\n",
      "Epoch 165, train loss: 0.2776 validation loss: 0.2996\n",
      "Epoch 166, train loss: 0.2858 validation loss: 0.3050\n",
      "Epoch 167, train loss: 0.2868 validation loss: 0.3040\n",
      "Epoch 168, train loss: 0.2851 validation loss: 0.3002\n",
      "Epoch 169, train loss: 0.2832 validation loss: 0.3000\n",
      "Epoch 170, train loss: 0.2805 validation loss: 0.3037\n",
      "Epoch 171, train loss: 0.2865 validation loss: 0.3017\n",
      "Epoch 172, train loss: 0.2847 validation loss: 0.3033\n",
      "Epoch 173, train loss: 0.2830 validation loss: 0.3034\n",
      "Epoch 174, train loss: 0.2842 validation loss: 0.3041\n",
      "Epoch 175, train loss: 0.2804 validation loss: 0.3031\n",
      "Epoch 176, train loss: 0.2845 validation loss: 0.3058\n",
      "Epoch 177, train loss: 0.2800 validation loss: 0.3031\n",
      "Epoch 178, train loss: 0.2804 validation loss: 0.3020\n",
      "Epoch 179, train loss: 0.2831 validation loss: 0.3013\n",
      "Epoch 180, train loss: 0.2794 validation loss: 0.3050\n",
      "Epoch 181, train loss: 0.2778 validation loss: 0.3002\n",
      "Epoch 182, train loss: 0.2820 validation loss: 0.3028\n",
      "Epoch 183, train loss: 0.2746 validation loss: 0.2989\n",
      "Epoch 184, train loss: 0.2810 validation loss: 0.3024\n",
      "Epoch 185, train loss: 0.2791 validation loss: 0.3002\n",
      "Epoch 186, train loss: 0.2809 validation loss: 0.3016\n",
      "Epoch 187, train loss: 0.2801 validation loss: 0.3029\n",
      "Epoch 188, train loss: 0.2753 validation loss: 0.3016\n",
      "Epoch 189, train loss: 0.2778 validation loss: 0.3017\n",
      "Epoch 190, train loss: 0.2755 validation loss: 0.3048\n",
      "Epoch 191, train loss: 0.2780 validation loss: 0.2999\n",
      "Epoch 192, train loss: 0.2796 validation loss: 0.3032\n",
      "Epoch 193, train loss: 0.2753 validation loss: 0.3046\n",
      "Epoch 194, train loss: 0.2794 validation loss: 0.3078\n",
      "Epoch 195, train loss: 0.2794 validation loss: 0.3028\n",
      "Epoch 196, train loss: 0.2749 validation loss: 0.3009\n",
      "Epoch 197, train loss: 0.2779 validation loss: 0.2942\n",
      "Epoch 198, train loss: 0.2773 validation loss: 0.3019\n",
      "Epoch 199, train loss: 0.2763 validation loss: 0.3000\n",
      "Epoch 200, train loss: 0.2754 validation loss: 0.2994\n",
      "Epoch 201, train loss: 0.2750 validation loss: 0.3002\n",
      "Epoch 202, train loss: 0.2783 validation loss: 0.3016\n",
      "Epoch 203, train loss: 0.2764 validation loss: 0.2995\n",
      "Epoch 204, train loss: 0.2739 validation loss: 0.2997\n",
      "Epoch 205, train loss: 0.2786 validation loss: 0.3015\n",
      "Epoch 206, train loss: 0.2734 validation loss: 0.2974\n",
      "Epoch 207, train loss: 0.2763 validation loss: 0.3047\n",
      "Epoch 208, train loss: 0.2764 validation loss: 0.3033\n",
      "Epoch 209, train loss: 0.2743 validation loss: 0.3052\n",
      "Epoch 210, train loss: 0.2729 validation loss: 0.2962\n",
      "Epoch 211, train loss: 0.2742 validation loss: 0.2951\n",
      "Epoch 212, train loss: 0.2714 validation loss: 0.3028\n",
      "Epoch 213, train loss: 0.2715 validation loss: 0.2990\n",
      "Epoch 214, train loss: 0.2717 validation loss: 0.2979\n",
      "Epoch 215, train loss: 0.2746 validation loss: 0.3032\n",
      "Epoch 216, train loss: 0.2739 validation loss: 0.3044\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[20], line 54\u001B[0m\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch_idx \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_batches):\n\u001B[1;32m     53\u001B[0m     x, y \u001B[38;5;241m=\u001B[39m get_batch(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mval\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m---> 54\u001B[0m     y_hat \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     55\u001B[0m     y \u001B[38;5;241m=\u001B[39m y\u001B[38;5;241m.\u001B[39mfloat()\n\u001B[1;32m     56\u001B[0m     loss_value \u001B[38;5;241m=\u001B[39m loss_fn(y_hat, y)\n",
      "File \u001B[0;32m~/mambaforge/envs/cats/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[0;32mIn[20], line 15\u001B[0m, in \u001B[0;36mTopicSegmentationModel.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m     14\u001B[0m     x_real \u001B[38;5;241m=\u001B[39m x[:,\u001B[38;5;241m0\u001B[39m,:,:]\u001B[38;5;241m.\u001B[39mreshape(batch_size, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m---> 15\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_real\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     16\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mactivation(x)\n\u001B[1;32m     17\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlinear2(x)\n",
      "File \u001B[0;32m~/mambaforge/envs/cats/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/mambaforge/envs/cats/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# create model\n",
    "import torch\n",
    "\n",
    "class TopicSegmentationModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TopicSegmentationModel, self).__init__()\n",
    "\n",
    "        self.linear1 = torch.nn.Linear(block_size*embedding_size, block_size)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(block_size, block_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_real = x[:,0,:,:].reshape(batch_size, -1)\n",
    "        x = self.linear1(x_real)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "model = TopicSegmentationModel()\n",
    "model.to(device)\n",
    "\n",
    "# create optimizer\n",
    "from transformers import AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# training loop\n",
    "from tqdm import tqdm\n",
    "\n",
    "num_epochs = 1000\n",
    "num_batches = len(train_data) // batch_size\n",
    "for epoch in range(num_epochs):\n",
    "    # train on training set\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx in range(num_batches):\n",
    "        x, y = get_batch('train')\n",
    "        optimizer.zero_grad()\n",
    "        y_hat = model(x)\n",
    "        y = y.float()\n",
    "        loss_value = loss_fn(y_hat, y)\n",
    "        train_loss += loss_value.item()\n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "    train_loss /= num_batches\n",
    "    # evaluate on validation set\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx in range(num_batches):\n",
    "            x, y = get_batch('val')\n",
    "            y_hat = model(x)\n",
    "            y = y.float()\n",
    "            loss_value = loss_fn(y_hat, y)\n",
    "            test_loss += loss_value.item()\n",
    "    test_loss /= num_batches\n",
    "    # print losses with .4 precision\n",
    "\n",
    "    print(f'Epoch {epoch}, train loss: {train_loss:.4f} validation loss: {test_loss:.4f}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-15T05:07:56.138985Z",
     "start_time": "2023-05-15T05:07:55.707537Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
