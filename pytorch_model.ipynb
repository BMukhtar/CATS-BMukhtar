{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-05-16T05:56:24.342659Z",
     "start_time": "2023-05-16T05:56:15.118705Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/Users/bm/.cache/huggingface/datasets/json/default-14dc56ae28f0b495/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e876ce86a86a471698667a4f424716a3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /Users/bm/.cache/huggingface/datasets/json/default-14dc56ae28f0b495/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-e65036e96329467f.arrow and /Users/bm/.cache/huggingface/datasets/json/default-14dc56ae28f0b495/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-507da4f4a54b4983.arrow\n",
      "Loading cached processed dataset at /Users/bm/.cache/huggingface/datasets/json/default-14dc56ae28f0b495/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-428753f94b90e10d.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": "Saving the dataset (0/1 shards):   0%|          | 0/100 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0f66f32177434a7aafb9d5705b44fd54"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "from datasets import load_dataset  # huggingface datasets\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files='./data/json/dev.json')\n",
    "# take only first 100 examples\n",
    "split_dataset = dataset['train'].train_test_split(test_size=0.1, seed=2357, shuffle=True)\n",
    "split_dataset['val'] = split_dataset.pop('test')  # rename the test split to val\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model_name = 'all-MiniLM-L6-v2'\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "\n",
    "# we now want to tokenize the dataset. first define the encoding function (gpt2 bpe)\n",
    "def process(example):\n",
    "    embds = model.encode(example['sentences'])\n",
    "    out = {'id': example['id'], 'labels': example['labels'], 'embeddings': embds, 'len': len(embds)}\n",
    "    return out\n",
    "\n",
    "\n",
    "# tokenize the dataset\n",
    "train_dataset = split_dataset['train'].select(range(100)).map(\n",
    "    process,\n",
    "    # remove_columns=['sentences'],\n",
    "    desc=\"tokenizing the splits\",\n",
    "    num_proc=1,\n",
    ")\n",
    "# save the tokenized dataset\n",
    "train_dataset.save_to_disk('./data/tokenized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Convert sentence embeddings and labels for trainable blocks\n",
    "\"\"\"\n",
    "import config\n",
    "import random\n",
    "fake_sent_embedding = model.encode([config.fake_sent])[0]\n",
    "embedding_size = model.get_sentence_embedding_dimension()\n",
    "test = False\n",
    "\n",
    "def create_fake_block(block, lines):\n",
    "    block_fake = block.copy()\n",
    "    random.shuffle(block_fake)\n",
    "    p = random.random()\n",
    "    if p >= 0.5:\n",
    "        for i in range(len(block_fake)):\n",
    "            p = random.random()\n",
    "            if p >= 0.5:\n",
    "                l = lines[random.randint(0, len(lines) - 1)]\n",
    "                block_fake[i] = (l, 0)\n",
    "    return block_fake\n",
    "\n",
    "def create_one_instance(block, lines):\n",
    "    records = []\n",
    "    fake_block = create_fake_block(block, lines)\n",
    "\n",
    "    blocks = list(zip(block, fake_block))\n",
    "    for item in blocks:\n",
    "        real_sentence = item[0][0]\n",
    "        real_label = item[0][1]\n",
    "        fake_sentence = item[1][0]\n",
    "        records.append((real_sentence,fake_sentence, real_label))\n",
    "    return records\n",
    "\n",
    "all_blocks = []\n",
    "\n",
    "for example in train_dataset:\n",
    "    labels = example['labels']\n",
    "    embds = example['embeddings']\n",
    "    raw_blocks = []\n",
    "    stride = 1 if test else config.sent_stride\n",
    "    i = 0\n",
    "    idx = 0\n",
    "    while i < len(labels):\n",
    "        block = [(embd, label) for embd, label in zip(embds[i:i + config.sent_window], labels[i:i + config.sent_window])]\n",
    "        if len(block) < config.sent_window:\n",
    "            block.extend([(fake_sent_embedding, 0)] * (config.sent_window - len(block)))\n",
    "        raw_blocks.append(block)\n",
    "        i += stride\n",
    "\n",
    "    if not test:\n",
    "        raw_blocks = raw_blocks[:int(config.perc_blocks_train * len(raw_blocks))]\n",
    "        random.shuffle(raw_blocks)\n",
    "\n",
    "    doc_recs = []\n",
    "    for rb in raw_blocks:\n",
    "        records = create_one_instance(rb, embds)\n",
    "        doc_recs.extend(records)\n",
    "\n",
    "    # save doc_recs to numpy array\n",
    "    all_blocks.extend(doc_recs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T05:56:24.642354Z",
     "start_time": "2023-05-16T05:56:24.259979Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "2960"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_blocks)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T05:56:24.738187Z",
     "start_time": "2023-05-16T05:56:24.641798Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "writing ./data/processed/train.bin: 3it [00:00, 47.15it/s]\n",
      "writing ./data/processed/val.bin: 3it [00:00, 47.41it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "dtype = np.dtype([('real', np.float32, embedding_size), ('fake', np.float32, embedding_size), ('label', np.int8)])\n",
    "data_dir = \"./data/processed\"\n",
    "def save_to_numpy(blocks, filename):\n",
    "    # create file if it doesn't exist\n",
    "    if not os.path.exists(os.path.dirname(filename)):\n",
    "        os.makedirs(os.path.dirname(filename))\n",
    "    # save all_blocks to numpy array\n",
    "    arr = np.memmap(filename, dtype=dtype, mode='w+', shape=(len(blocks),))\n",
    "\n",
    "    # create batches of blocks for faster write\n",
    "    batch_size = 1024\n",
    "    batches = [blocks[i:i + batch_size] for i in range(0, len(blocks), batch_size)]\n",
    "    for batch_idx, batch in tqdm(enumerate(batches), desc=f'writing {filename}'):\n",
    "        arr_batch = np.array(batch, dtype=dtype)\n",
    "        arr[batch_idx * batch_size : batch_idx * batch_size + len(arr_batch)] = arr_batch\n",
    "    arr.flush()\n",
    "\n",
    "save_to_numpy(all_blocks, os.path.join(data_dir, 'train.bin'))\n",
    "save_to_numpy(all_blocks, os.path.join(data_dir, 'val.bin'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T05:56:24.983507Z",
     "start_time": "2023-05-16T05:56:24.647463Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "2960"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the numpy array from disk\n",
    "train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=dtype, mode='r')\n",
    "len(train_data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T05:56:24.985314Z",
     "start_time": "2023-05-16T05:56:24.839220Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "batch_size = 12 # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
    "block_size = config.sent_window\n",
    "device = 'cpu' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "\n",
    "\n",
    "train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=dtype, mode='r')\n",
    "val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=dtype, mode='r')\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in ix:\n",
    "        batch_numpy = data[i:i+block_size]\n",
    "        # given numpy array strides not a multiple of the element byte size. Copy the numpy array to reallocate the memory.\n",
    "        batch_numpy = np.copy(batch_numpy)\n",
    "        x.append(torch.stack([torch.from_numpy( np.copy(batch_numpy['real'])), torch.from_numpy( np.copy(batch_numpy['fake']))]))\n",
    "        y.append(torch.from_numpy(batch_numpy['label']))\n",
    "    x = torch.stack(x)\n",
    "    y = torch.stack(y)\n",
    "    if device_type == 'cuda':\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T05:56:24.986996Z",
     "start_time": "2023-05-16T05:56:24.839598Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming training from ./model_checkpoints/ckpt.pt\n",
      "epoch 0 train loss 0.3298 val loss 0.2965\n",
      "saving checkpoint to ./model_checkpoints/ckpt.pt\n",
      "epoch 10 train loss 0.3044 val loss 0.3213\n",
      "saving checkpoint to ./model_checkpoints/ckpt.pt\n",
      "epoch 20 train loss 0.2983 val loss 0.3094\n",
      "saving checkpoint to ./model_checkpoints/ckpt.pt\n",
      "epoch 30 train loss 0.2759 val loss 0.2859\n",
      "saving checkpoint to ./model_checkpoints/ckpt.pt\n",
      "epoch 40 train loss 0.2894 val loss 0.2863\n",
      "saving checkpoint to ./model_checkpoints/ckpt.pt\n",
      "epoch 50 train loss 0.3099 val loss 0.2807\n",
      "saving checkpoint to ./model_checkpoints/ckpt.pt\n",
      "epoch 60 train loss 0.2853 val loss 0.2840\n",
      "saving checkpoint to ./model_checkpoints/ckpt.pt\n",
      "epoch 70 train loss 0.2986 val loss 0.2656\n",
      "saving checkpoint to ./model_checkpoints/ckpt.pt\n",
      "epoch 80 train loss 0.2774 val loss 0.2939\n",
      "saving checkpoint to ./model_checkpoints/ckpt.pt\n",
      "epoch 90 train loss 0.2933 val loss 0.2876\n",
      "saving checkpoint to ./model_checkpoints/ckpt.pt\n"
     ]
    }
   ],
   "source": [
    "sample_batch = get_batch('train')\n",
    "assert sample_batch[0].shape == (batch_size, 2, block_size, embedding_size)\n",
    "assert sample_batch[1].shape == (batch_size, block_size)\n",
    "\n",
    "# create model\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AdamW\n",
    "\n",
    "class TopicSegmentationModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TopicSegmentationModel, self).__init__()\n",
    "\n",
    "        self.linear1 = torch.nn.Linear(block_size*embedding_size, block_size)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(block_size, block_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_real = x[:,0,:,:].reshape(batch_size, -1)\n",
    "        x = self.linear1(x_real)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "eval_iters = 10\n",
    "eval_interval = 10\n",
    "num_epochs = 100\n",
    "best_val_loss = 1e9\n",
    "ckpt_path = './model_checkpoints/ckpt.pt'\n",
    "num_batches = len(train_data) // batch_size\n",
    "always_save_checkpoint = True\n",
    "init_from = \"resume\"\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "model = TopicSegmentationModel()\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "if init_from == 'scratch':\n",
    "    # init a new model from scratch\n",
    "    print(\"Initializing a new model from scratch\")\n",
    "    # determine the vocab size we'll use for from-scratch training\n",
    "    optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "else:\n",
    "    print(f\"Resuming training from {ckpt_path}\")\n",
    "    # resume training from a checkpoint.\n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "    # force these config attributes to be equal otherwise we can't even resume training\n",
    "    # the rest of the attributes (e.g. dropout) can stay as desired from command line\n",
    "\n",
    "    # create the model\n",
    "    optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "    epoch = checkpoint['iter_num']\n",
    "\n",
    "    state_dict = checkpoint['model']\n",
    "    # fix the keys of the state dictionary :(\n",
    "    # honestly no idea how checkpoints sometimes get this prefix, have to debug more\n",
    "    unwanted_prefix = '_orig_mod.'\n",
    "    for k,v in list(state_dict.items()):\n",
    "        if k.startswith(unwanted_prefix):\n",
    "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "    model.load_state_dict(state_dict)\n",
    "    iter_num = checkpoint['iter_num']\n",
    "    best_val_loss = checkpoint['best_val_loss']\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            x, y = get_batch(split)\n",
    "            y_hat = model(x)\n",
    "            y = y.float()\n",
    "            loss_value = loss_fn(y_hat, y)\n",
    "            losses[k] = loss_value\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train on training set\n",
    "    # evaluate the loss on train/val sets and write checkpoints\n",
    "    if epoch % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        if losses['val'] < best_val_loss or always_save_checkpoint:\n",
    "            best_val_loss = losses['val']\n",
    "            # print losses\n",
    "            print(f\"epoch {epoch} train loss {losses['train']:.4f} val loss {losses['val']:.4f}\")\n",
    "            if epoch > -1:\n",
    "                checkpoint = {\n",
    "                    'model': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'iter_num': epoch,\n",
    "                    'best_val_loss': best_val_loss,\n",
    "                }\n",
    "                print(f\"saving checkpoint to {ckpt_path}\")\n",
    "                torch.save(checkpoint, ckpt_path)\n",
    "    model.train()\n",
    "    for batch_idx in range(num_batches):\n",
    "        x, y = get_batch('train')\n",
    "        optimizer.zero_grad()\n",
    "        y_hat = model(x)\n",
    "        y = y.float()\n",
    "        loss_value = loss_fn(y_hat, y)\n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "    #\n",
    "    # # evaluate on validation set\n",
    "    # model.eval()\n",
    "    # with torch.no_grad():\n",
    "    #     x, y = get_batch('val')\n",
    "    #     y_hat = model(x)\n",
    "    #     y = y.float()\n",
    "    #     loss_value = loss_fn(y_hat, y)\n",
    "    #     print(f'Epoch {epoch} validation loss: {loss_value.item()}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T08:01:33.005747Z",
     "start_time": "2023-05-16T08:01:09.494697Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
