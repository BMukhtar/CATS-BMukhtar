{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-05-18T20:09:07.124064Z",
     "start_time": "2023-05-18T20:09:02.555282Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.53s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": "Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "83cdb94300c7459c92b86bd7957d2b2c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e3e67e5bb5b6469aa047b51fc1406f12"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Saving the dataset (0/1 shards):   0%|          | 0/50 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "742cce18e14b4be4962e5f551637df3d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/Users/make/.cache/huggingface/datasets/json/default-00ff1c561751f0d8/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2fb696065fd14756ad8a1844f551a9f4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /Users/make/.cache/huggingface/datasets/json/default-00ff1c561751f0d8/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-47356b4111c2d823.arrow and /Users/make/.cache/huggingface/datasets/json/default-00ff1c561751f0d8/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-bfe10a35ef1eedaa.arrow\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.35it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/input\t0\n",
      "Found 50 text files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 5895.51it/s]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.52s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": "Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3900afcd083b4c61b3043d4958694570"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0490ba6c96dc4845a5b7be9b77be6b1b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Saving the dataset (0/1 shards):   0%|          | 0/50 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "31f388b365bf4980aaa062f3e1075a3e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import datasets\n",
    "from datasets import load_dataset  # huggingface datasets\n",
    "\n",
    "import utils\n",
    "import config\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "def sentence_split(text):\n",
    "    return sent_tokenize(text)\n",
    "\n",
    "import os\n",
    "def create_instances(dir_path, title_start = \"========,\", ssplit = True):\n",
    "    files = []\n",
    "    utils.get_files_recursive(dir_path, files)\n",
    "    print(\"Found \" + str(len(files)) + \" text files.\")\n",
    "    res = {\n",
    "        'id': [],\n",
    "        'labels': [],\n",
    "        'sentences': [],\n",
    "    }\n",
    "    for f in tqdm(files):\n",
    "        example = process_document(f, title_start = title_start, ssplit = ssplit)\n",
    "        res['id'].append(example['id'])\n",
    "        res['labels'].append(example['labels'])\n",
    "        res['sentences'].append(example['sentences'])\n",
    "    return res\n",
    "\n",
    "\n",
    "def process_document(path, title_start = \"========,\", forbidden_start = \"***LIST***\", ssplit = True):\n",
    "    lines = ([l for l in utils.load_lines(path) if not l.startswith(forbidden_start)]) if ssplit else (sentence_split(utils.load_txt_file(path)))\n",
    "    labels = []\n",
    "    sentences = []\n",
    "    for i in range(len(lines)):\n",
    "        if lines[i].startswith(title_start):\n",
    "            continue\n",
    "        if (i-1) >= 0 and lines[i-1].startswith(title_start):\n",
    "            sentences.append(lines[i])\n",
    "            labels.append(1)\n",
    "        else:\n",
    "            sentences.append(lines[i])\n",
    "            labels.append(0)\n",
    "\n",
    "    return {\n",
    "        'id': path,\n",
    "        'labels': labels,\n",
    "        'sentences': sentences,\n",
    "    }\n",
    "\n",
    "def get_training_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device('mps')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    return device\n",
    "\n",
    "device = get_training_device()\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files='./data/json/dev.json')\n",
    "# take only first 100 examples\n",
    "split_dataset = dataset['train'].train_test_split(test_size=0.1, seed=2357, shuffle=True)\n",
    "split_dataset['val'] = split_dataset.pop('test')  # rename the test split to val\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model_name = 'all-MiniLM-L6-v2'\n",
    "sentence_model = SentenceTransformer(model_name, device=device)\n",
    "\n",
    "# we now want to tokenize the dataset. first define the encoding function (gpt2 bpe)\n",
    "def process(example):\n",
    "    embds = sentence_model.encode(example['sentences'])\n",
    "    out = {\n",
    "        'id': example['id'],\n",
    "        'labels': example['labels'],\n",
    "        'sentences': example['sentences'],\n",
    "        'embeddings': embds,\n",
    "        'len': len(embds)\n",
    "    }\n",
    "    return out\n",
    "\n",
    "def preprocess_list_data(data):\n",
    "    # for avoiding long time, we process the dataset in batches for parallel processing\n",
    "    batch_size = 512\n",
    "    all_processed = {\n",
    "        'id': [],\n",
    "        'labels': [],\n",
    "        'sentences': [],\n",
    "        'embeddings': [],\n",
    "        'len': []\n",
    "    }\n",
    "    batch_ids = [list(range(i, min(i + batch_size, len(data)))) for i in range(0, len(data), batch_size)]\n",
    "    for batch_id in tqdm(batch_ids):\n",
    "        batch = data.select(batch_id)\n",
    "        all_sentences = []\n",
    "        for example in batch:\n",
    "            all_sentences.extend(example['sentences'])\n",
    "        embs = sentence_model.encode(all_sentences)\n",
    "        offset = 0\n",
    "        for example in batch:\n",
    "            next_offset = offset + len(example['sentences'])\n",
    "            embds = embs[offset:offset + next_offset]\n",
    "            offset = next_offset\n",
    "            all_processed['id'].append(example['id'])\n",
    "            all_processed['labels'].append(example['labels'])\n",
    "            all_processed['sentences'].append(example['sentences'])\n",
    "            all_processed['embeddings'].append(embds)\n",
    "            all_processed['len'].append(len(embds))\n",
    "\n",
    "    return all_processed\n",
    "\n",
    "\n",
    "def preprocess_dataset(dataset: datasets.Dataset):\n",
    "    # for avoiding long time, we process the dataset in batches for parallel processing\n",
    "    return datasets.Dataset.from_dict(preprocess_list_data(dataset))\n",
    "\n",
    "train_dataset = preprocess_dataset(split_dataset['train'].select(range(10)))\n",
    "val_dataset = preprocess_dataset(split_dataset['val'].select(range(10)))\n",
    "test_dataset = preprocess_dataset(datasets.Dataset.from_dict(create_instances(dir_path='./data/input')))\n",
    "\n",
    "\n",
    "# save the tokenized dataset\n",
    "train_dataset.save_to_disk('./data/tokenized_train')\n",
    "val_dataset.save_to_disk('./data/tokenized_val')\n",
    "test_dataset.save_to_disk('./data/tokenized_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Convert sentence embeddings and labels for trainable blocks\n",
    "\"\"\"\n",
    "import config\n",
    "import random\n",
    "\n",
    "fake_sent_embedding = sentence_model.encode([config.fake_sent])[0]\n",
    "embedding_size = sentence_model.get_sentence_embedding_dimension()\n",
    "\n",
    "\n",
    "def create_fake_block(block, lines):\n",
    "    block_fake = block.copy()\n",
    "    random.shuffle(block_fake)\n",
    "    p = random.random()\n",
    "    if p >= 0.5:\n",
    "        for i in range(len(block_fake)):\n",
    "            p = random.random()\n",
    "            if p >= 0.5:\n",
    "                l = lines[random.randint(0, len(lines) - 1)]\n",
    "                block_fake[i] = (l, 0)\n",
    "    return block_fake\n",
    "\n",
    "\n",
    "def create_one_instance(block, lines):\n",
    "    records = []\n",
    "    fake_block = create_fake_block(block, lines)\n",
    "\n",
    "    blocks = list(zip(block, fake_block))\n",
    "    for item in blocks:\n",
    "        real_sentence = item[0][0]\n",
    "        real_label = item[0][1]\n",
    "        fake_sentence = item[1][0]\n",
    "        records.append((real_sentence, fake_sentence, real_label))\n",
    "    return records\n",
    "\n",
    "\n",
    "def get_blocks(dataset, test=False):\n",
    "    all_blocks = []\n",
    "    for example in dataset:\n",
    "        labels = example['labels']\n",
    "        embds = example['embeddings']\n",
    "        raw_blocks = []\n",
    "        stride = 1 if test else config.sent_stride\n",
    "        i = 0\n",
    "        while i < len(labels):\n",
    "            block = [(embd, label) for embd, label in\n",
    "                     zip(embds[i:i + config.sent_window], labels[i:i + config.sent_window])]\n",
    "            if len(block) < config.sent_window:\n",
    "                block.extend([(fake_sent_embedding, 0)] * (config.sent_window - len(block)))\n",
    "            raw_blocks.append(block)\n",
    "            i += stride\n",
    "\n",
    "        if not test:\n",
    "            raw_blocks = raw_blocks[:int(config.perc_blocks_train * len(raw_blocks))]\n",
    "            random.shuffle(raw_blocks)\n",
    "\n",
    "        doc_recs = []\n",
    "        for rb in raw_blocks:\n",
    "            records = create_one_instance(rb, embds)\n",
    "            doc_recs.extend(records)\n",
    "\n",
    "        # save doc_recs to numpy array\n",
    "        all_blocks.extend(doc_recs)\n",
    "    return all_blocks\n",
    "\n",
    "\n",
    "train_blocks = get_blocks(train_dataset)\n",
    "val_blocks = get_blocks(val_dataset, test=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "dtype = np.dtype([('real', np.float32, embedding_size), ('fake', np.float32, embedding_size), ('label', np.int8)])\n",
    "data_dir = \"./data/processed\"\n",
    "\n",
    "\n",
    "def save_to_numpy(blocks, filename):\n",
    "    # create file if it doesn't exist\n",
    "    if not os.path.exists(os.path.dirname(filename)):\n",
    "        os.makedirs(os.path.dirname(filename))\n",
    "    # save all_blocks to numpy array\n",
    "    arr = np.memmap(filename, dtype=dtype, mode='w+', shape=(len(blocks),))\n",
    "\n",
    "    # create batches of blocks for faster write\n",
    "    batch_size = 1024\n",
    "    batches = [blocks[i:i + batch_size] for i in range(0, len(blocks), batch_size)]\n",
    "    for batch_idx, batch in tqdm(enumerate(batches), desc=f'writing {filename}'):\n",
    "        arr_batch = np.array(batch, dtype=dtype)\n",
    "        arr[batch_idx * batch_size: batch_idx * batch_size + len(arr_batch)] = arr_batch\n",
    "    arr.flush()\n",
    "\n",
    "\n",
    "save_to_numpy(train_blocks, os.path.join(data_dir, 'train.bin'))\n",
    "save_to_numpy(val_blocks, os.path.join(data_dir, 'val.bin'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load the numpy array from disk\n",
    "train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=dtype, mode='r')\n",
    "val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=dtype, mode='r')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "batch_size = 12  # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
    "block_size = config.sent_window\n",
    "\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu'  # for later use in torch.autocast\n",
    "\n",
    "train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=dtype, mode='r')\n",
    "val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=dtype, mode='r')\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in ix:\n",
    "        batch_numpy = data[i:i + block_size]\n",
    "        # given numpy array strides not a multiple of the element byte size. Copy the numpy array to reallocate the memory.\n",
    "        batch_numpy = np.copy(batch_numpy)\n",
    "        x.append(torch.stack(\n",
    "            [torch.from_numpy(np.copy(batch_numpy['real'])), torch.from_numpy(np.copy(batch_numpy['fake']))]))\n",
    "        y.append(torch.from_numpy(batch_numpy['label']))\n",
    "    x = torch.stack(x)\n",
    "    y = torch.stack(y)\n",
    "    if device_type == 'cuda':\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sample_batch = get_batch('train')\n",
    "assert sample_batch[0].shape == (batch_size, 2, block_size, embedding_size)\n",
    "assert sample_batch[1].shape == (batch_size, block_size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create model\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AdamW\n",
    "\n",
    "\n",
    "class TopicSegmentationModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TopicSegmentationModel, self).__init__()\n",
    "\n",
    "        self.linear1 = torch.nn.Linear(block_size * embedding_size, block_size)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(block_size, block_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_real = x[:, 0, :, :].reshape(batch_size, -1)\n",
    "        x = self.linear1(x_real)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "eval_iters = 10\n",
    "eval_interval = 10\n",
    "num_epochs = 1\n",
    "best_val_loss = 1e9\n",
    "ckpt_path = './model_checkpoints/ckpt.pt'\n",
    "num_batches = len(train_data) // batch_size\n",
    "always_save_checkpoint = True\n",
    "init_from = \"scratch\"\n",
    "epoch_offset = 0\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "model = TopicSegmentationModel()\n",
    "sentence_model.to(device)\n",
    "\n",
    "if init_from == 'scratch':\n",
    "    # init a new model from scratch\n",
    "    print(\"Initializing a new model from scratch\")\n",
    "    # determine the vocab size we'll use for from-scratch training\n",
    "    optimizer = AdamW(sentence_model.parameters(), lr=1e-5)\n",
    "else:\n",
    "    print(f\"Resuming training from {ckpt_path}\")\n",
    "    # resume training from a checkpoint.\n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "    # force these config attributes to be equal otherwise we can't even resume training\n",
    "    # the rest of the attributes (e.g. dropout) can stay as desired from command line\n",
    "\n",
    "    # create the model\n",
    "    optimizer = AdamW(sentence_model.parameters(), lr=1e-5)\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "    epoch = checkpoint['iter_num']\n",
    "\n",
    "    state_dict = checkpoint['model']\n",
    "    epoch_offset = checkpoint['epoch_offset']\n",
    "    # fix the keys of the state dictionary :(\n",
    "    # honestly no idea how checkpoints sometimes get this prefix, have to debug more\n",
    "    unwanted_prefix = '_orig_mod.'\n",
    "    for k, v in list(state_dict.items()):\n",
    "        if k.startswith(unwanted_prefix):\n",
    "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "    model.load_state_dict(state_dict)\n",
    "    iter_num = checkpoint['iter_num']\n",
    "    best_val_loss = checkpoint['best_val_loss']\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    sentence_model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            x, y = get_batch(split)\n",
    "            y_hat = model(x)\n",
    "            y = y.float()\n",
    "            loss_value = loss_fn(y_hat, y)\n",
    "            losses[k] = loss_value\n",
    "        out[split] = losses.mean()\n",
    "    sentence_model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "for epoch in range(epoch_offset, epoch_offset + num_epochs):\n",
    "    # train on training set\n",
    "    # evaluate the loss on train/val sets and write checkpoints\n",
    "    if epoch % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        if losses['val'] < best_val_loss or always_save_checkpoint:\n",
    "            best_val_loss = losses['val']\n",
    "            # print losses\n",
    "            print(f\"epoch {epoch} train loss {losses['train']:.4f} val loss {losses['val']:.4f}\")\n",
    "            if epoch > -1:\n",
    "                checkpoint = {\n",
    "                    'model': sentence_model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'iter_num': epoch,\n",
    "                    'best_val_loss': best_val_loss,\n",
    "                    'epoch_offset': epoch,\n",
    "                }\n",
    "                print(f\"saving checkpoint to {ckpt_path}\")\n",
    "                torch.save(checkpoint, ckpt_path)\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx in range(num_batches):\n",
    "        x, y = get_batch('train')\n",
    "        optimizer.zero_grad()\n",
    "        y_hat = model(x)\n",
    "        y = y.float()\n",
    "        loss_value = loss_fn(y_hat, y)\n",
    "        train_loss += loss_value.item()\n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "    #\n",
    "    # # evaluate on validation set\n",
    "    # model.eval()\n",
    "    # with torch.no_grad():\n",
    "    #     x, y = get_batch('val')\n",
    "    #     y_hat = model(x)\n",
    "    #     y = y.float()\n",
    "    #     loss_value = loss_fn(y_hat, y)\n",
    "    #     print(f'Epoch {epoch} validation loss: {loss_value.item()}')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def predict(dataset, output_dir):\n",
    "    res = model.forward(dataset)\n",
    "\n",
    "    print(\"Documents to segment: \" + str(len(test_texts[0])))\n",
    "    flat_blocks = []\n",
    "    for x in test_texts[0]:\n",
    "        print(len(x[1]))\n",
    "        flat_blocks.extend(x[1])\n",
    "\n",
    "    print(\"Number of prediction blocks: \" + str(len(flat_blocks)))\n",
    "\n",
    "    print(\"Predicting with the model (this may take a while, depending on the number of documents)...\")\n",
    "    res_list = list(res)\n",
    "    print(\"Predictions completed.\")\n",
    "\n",
    "    thold = 0.3 if config.MODEL_TYPE == \"cats\" else 0.5\n",
    "\n",
    "    glob_cntr = 0\n",
    "    docs = test_texts[0]\n",
    "\n",
    "    agg_docs = []\n",
    "\n",
    "    for i in range(len(docs)):\n",
    "        fname = docs[i][0]\n",
    "        if i % 1000 == 1:\n",
    "            print(fname)\n",
    "            print(str(i) + \" of \" + str(len(docs)) + \" documents...\")\n",
    "        blocks = docs[i][1]\n",
    "        preds_blocks = res_list[glob_cntr: glob_cntr + len(blocks)]\n",
    "        glob_cntr += len(blocks)\n",
    "\n",
    "        sent_scores = [(b[0][0], b[0][1], []) for b in blocks]\n",
    "        for b_ind in range(len(blocks)):\n",
    "            for relb_ind in range(len(blocks[b_ind])):\n",
    "                if blocks[b_ind][relb_ind][0] == config.fake_sent:\n",
    "                    break\n",
    "                else:\n",
    "                    sent_ind = b_ind + relb_ind\n",
    "                    score = preds_blocks[b_ind][relb_ind][1]\n",
    "                    sent_scores[sent_ind][2].append(score)\n",
    "        agg_sent_scores = [(x[0], x[1], np.mean(x[2]), (1 if np.mean(x[2]) >= thold else 0)) for x in sent_scores]\n",
    "        agg_docs.append(agg_sent_scores)\n",
    "\n",
    "    # printing out predictions\n",
    "    docnames = [x[0] for x in docs]\n",
    "    print(\"Storing segmented texts...\")\n",
    "    docscores = zip(docnames, agg_docs)\n",
    "    for name, sentscores in docscores:\n",
    "        print(\"Document: \" + name)\n",
    "        lines = []\n",
    "        for s in sentscores:\n",
    "            if s[2] >= thold:\n",
    "                lines.append(config.seg_start)\n",
    "            lines.append(s[0] + \"\\t\" + str(s[2]) if write_pred_score else s[0])\n",
    "        utils.write_list(os.path.join(output_dir, name.split(\"/\")[-1] + \".seg\"), lines)\n",
    "    print(\"Stored.\")\n",
    "\n",
    "predict(test_dataset, \"./data/output\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
